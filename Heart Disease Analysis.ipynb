{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b5b4f3",
   "metadata": {},
   "source": [
    "# Coronary Heart Disease Prediction Using SuperLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897894c",
   "metadata": {},
   "source": [
    "## Import necessary libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa920542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow import keras\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d428309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\23480\\\\Documents\\\\Personal\\\\SHU Academic\\\\3rd Semester\\\\Dissertation\\\\Dataset\\\\heart+disease\\\\processed_cleveland.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fbd8e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9005f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to get an overview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numer of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of numerical features\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee806f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with '?' in 'ca' and 'thal' features\n",
    "df = df[df['ca'] != '?']\n",
    "df = df[df['thal'] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4446e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numer of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff58986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for data type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233665c",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Classifications and Feature Distributions\n",
    "\n",
    "# Define a list of numerical and categorical features\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', ]\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'ca']\n",
    "\n",
    "# Create subplots for numerical features\n",
    "num_rows = len(numerical_features)\n",
    "num_cols = 2\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    row, col = i // num_cols, i % num_cols\n",
    "    ax = axes[row, col]\n",
    "    df[feature].plot(kind='hist', ax=ax, legend=True)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "\n",
    "# Remove any empty subplot\n",
    "if len(numerical_features) < num_rows * num_cols:\n",
    "    for i in range(len(numerical_features), num_rows * num_cols):\n",
    "        fig.delaxes(axes[i // num_cols, i % num_cols])\n",
    "\n",
    "# Create bar plots for categorical features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 10))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    row, col = i // 4, i % 4\n",
    "    ax = axes[row, col]\n",
    "    df[feature].value_counts().plot(kind='bar', ax=ax, legend=True)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "\n",
    "# Remove any empty subplot\n",
    "if len(categorical_features) < 7:\n",
    "    for i in range(len(categorical_features), 7):\n",
    "        fig.delaxes(axes[i // 4, i % 4])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a1e5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(data= df, x='exang',hue='thal')\n",
    "plt.title('exang v/s Thalassemia\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only people with heart disease according to the dataset\n",
    "\n",
    "# Filter out rows with target values 1, 2, 3, or 4\n",
    "df2 = df[df['num'].isin([1, 2, 3, 4])]\n",
    "pal = sns.light_palette(\"blue\", as_cmap=True)\n",
    "print('Age vs trestbps(Resting Blood Pressure)')\n",
    "sns.jointplot(data=df2,\n",
    "              x='age',\n",
    "              y='trestbps',\n",
    "              kind='hex',\n",
    "              cmap='Reds'\n",
    "           \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2d75b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a joint plot for \"chol\" and \"cp\"\n",
    "sns.set(style=\"whitegrid\")  # Set the style of the plot\n",
    "sns.jointplot(data = df2, x=\"age\", y=\"thalach\", kind=\"kde\", height=7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e134154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a joint plot for \"chol\" and \"cp\"\n",
    "sns.set(style=\"whitegrid\")  # Set the style of the plot\n",
    "sns.jointplot(data = df2, x=\"cp\", y=\"chol\", kind=\"kde\", height=7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2085507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart disease severity as it impacts the sexes\n",
    "sns.countplot(data= df2, x='sex',hue='num')\n",
    "plt.title('Sex v/s Heart disease Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart diesae severity in light of chest pain\n",
    "sns.countplot(data= df2, x='cp',hue='num')\n",
    "plt.title('Chest Pain v/s Heart disease Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of coloured major blood vessels versus Heart disease severity\n",
    "sns.countplot(data= df2, x='ca',hue='num')\n",
    "plt.title('Number of Coloured Major Vessels v/s Heart disease Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable descriptions\n",
    "variable_descriptions = {\n",
    "    'sex': { 0: 'Female', 1: 'Male'},\n",
    "    'restecg': {0: 'Normal', 1: 'ST-T Wave Abnormality', 2: 'Left Ventricular Hypertrophy'},\n",
    "    'slope': {1: 'Upsloping', 2: 'Flat', 3: 'Downsloping'},\n",
    "    'thal': {3: 'Normal', 6: 'Fixed Defect', 7: 'Reversible Defect'},\n",
    "    'exang': {0: 'No', 1: 'Yes'},\n",
    "    'cp': {1:'Typical Anginal', 2: 'Atypical Anginal', 3: 'Non-Anginal Pain', 4:'Asymptomatic' },\n",
    "    'fbs': {0: 'No', 1: 'Yes'},\n",
    "    'num': {0: 'Normal', 1: 'SIHD', 2: 'Unstable Angina', 3: 'NSTEMI', 4: 'STEMI'}\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the variable descriptions\n",
    "variable_table = pd.DataFrame.from_dict(variable_descriptions, orient='columns')\n",
    "\n",
    "# Transpose the table for a more intuitive view\n",
    "variable_table = variable_table.T\n",
    "\n",
    "# Rename the index column for clarity\n",
    "variable_table.index.name = 'Feature'\n",
    "\n",
    "# Display the table\n",
    "print(variable_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c98ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation matrix heatmap\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the correlation matrix using seaborn\n",
    "sns.heatmap(correlation_matrix, \n",
    "        xticklabels=correlation_matrix.columns,\n",
    "        yticklabels=correlation_matrix.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a10c7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the categorical features with variability more than 0 and 1\n",
    "\n",
    "df['cp'].replace({1:'typical_angina', 2:'atypical_angina', 3:'non_anginal_pain', 4:'asymptomatic'}, inplace = True)\n",
    "df['restecg'].replace({0:'normal', 1:'ST-T_wave_abnormality', 2:'left_ventricular_hypertrophy'}, inplace = True)\n",
    "df['slope'].replace({1:'upsloping', 2:'flat', 3:'downsloping'}, inplace = True)\n",
    "df['thal'].replace({3:'normal', 6:'fixed_defect', 7:'reversible_defect'}, inplace = True)\n",
    "\n",
    "features = df.columns.to_list()\n",
    "categorical_features = ['cp', 'thal', 'restecg', 'slope']\n",
    "categorical_features = pd.get_dummies(df[categorical_features].applymap(str))\n",
    "features.remove('num')\n",
    "\n",
    "features.remove('cp')\n",
    "features.remove('thal')\n",
    "features.remove('restecg')\n",
    "features.remove('slope')\n",
    "\n",
    "\n",
    "y = df['num']\n",
    "y.columns = ['target']\n",
    "X = pd.concat([df[features],categorical_features], axis = 1)\n",
    "X.drop([92, 138, 163, 164, 251])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a431d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677f98c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a674e01",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot class distribution after applying SMOTE\n",
    "y.value_counts().plot(kind='bar')\n",
    "plt.xlabel(\"Target\")\n",
    "plt.title('Class Distribution before Applying SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Balancing Using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733626f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot class distribution after applying SMOTE\n",
    "y.value_counts().plot(kind='bar')\n",
    "plt.xlabel(\"Target\")\n",
    "plt.title('Class Distribution After Applying SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd545bf",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Recursive Feature Elimination (RFE) with cross-validation\n",
    "\n",
    "\n",
    "# Creating a random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Initialize the RFE with cross-validation\n",
    "rfecv = RFECV(estimator=clf, step=1, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fitting the RFE to the data\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# DataFrame to display the ranking, scores, and p-values for all features\n",
    "feature_ranking_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Feature ranking\": rfecv.support_,\n",
    "})\n",
    "print(feature_ranking_df)\n",
    "\n",
    "# Print the optimal number of features and their support\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(\"Feature ranking: \", rfecv.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af335c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Univariate Feature Selection Method: SelectKBest\n",
    "\n",
    "\n",
    "# Initialize SelectKBest with the scoring function\n",
    "selector = SelectKBest(score_func=chi2)\n",
    "\n",
    "# Fit data to compute the scores and p-values for all features\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get the scores and p-values for all features\n",
    "feature_scores = selector.scores_\n",
    "feature_pvalues = selector.pvalues_\n",
    "\n",
    "# DataFrame to display the ranking, scores, and p-values for all features\n",
    "feature_ranking_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Score\": feature_scores,\n",
    "    \"P-Value\": feature_pvalues\n",
    "})\n",
    "\n",
    "# Sorting the features by score in descending order\n",
    "feature_ranking_df = feature_ranking_df.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "print(feature_ranking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tree-based Feature importance\n",
    "\n",
    "# Creating a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting the classifier to the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Getting feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# DataFrame to display the features and their importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": feature_importances\n",
    "})\n",
    "\n",
    "# Sorting the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print or use the features based on their importance scores\n",
    "print(\"Feature Importance Scores:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Adjusting the threshold to select the top features based on importance\n",
    "threshold = 0.01\n",
    "selected_features = feature_importance_df[feature_importance_df['Importance'] >= threshold]['Feature']\n",
    "X_selected = X[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a377a3",
   "metadata": {},
   "source": [
    "## Testing Selected Features with Relevant ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584110c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Using RFC\n",
    "\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rfc.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Display a classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XG Boost\n",
    "\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data with the selected features\n",
    "xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgb.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Display a classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Support Vector Machine\n",
    "\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Fit the model to the training data with the selected features\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Display a classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Neural Network\n",
    "\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Create a simple feedforward neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_selected, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Generate a confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_classes)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Display a classification report\n",
    "classification_rep = classification_report(y_test, y_pred_classes)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836f846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using KNN\n",
    "\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Create a KNN classifier with a specified number of neighbors (e.g., 5)\n",
    "k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the KNN model on the training data\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test_selected)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print a classification report with precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create and print a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Logistic Regression\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex','trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca', 'cp_asymptomatic', \n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7', \n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping']\n",
    "\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "#Train-Test dataset split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Perform data normalization (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Logistic Regression model\n",
    "lrg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lrg.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lrg.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model for multiclass classification\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print a classification report with precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'cp_asymptomatic',\n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7',\n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping', 'ca']\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Convert 'ca' feature to float\n",
    "X_selected['ca'] = X_selected['ca'].astype(float)\n",
    "\n",
    "# Train-Test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Perform data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Define base learners\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(learning_rate=0.1,objective=\"multi:softmax\", n_estimators=100, random_state=42)\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "nn = MLPClassifier(hidden_layer_sizes=(128, 128, 128), max_iter=500, random_state=42)\n",
    "\n",
    "# Create a SuperLearner class\n",
    "class SuperLearner(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_learners):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    def fit(self, X_train_selected, y):\n",
    "        for model in self.base_learners:\n",
    "            model.fit(X_train_selected, y)\n",
    "        predictions = np.column_stack([model.predict(X_train_selected) for model in self.base_learners])\n",
    "        self.meta_learner.fit(predictions, y)\n",
    "\n",
    "    def predict(self, X_train_selected):\n",
    "        predictions = np.column_stack([model.predict(X_train_selected) for model in self.base_learners])\n",
    "        return self.meta_learner.predict(predictions)\n",
    "\n",
    "# Initialize the Super Learner with the base learners\n",
    "super_learner = SuperLearner(base_learners=[knn, rfc, xgb, svm, nn])\n",
    "\n",
    "# Implement k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_selected):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fit the Super Learner on the training data\n",
    "    super_learner.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_cv = super_learner.predict(X_test_cv)\n",
    "\n",
    "    # Calculate and store the accuracy score\n",
    "    accuracy_cv = accuracy_score(y_test_cv, y_pred_cv)\n",
    "    accuracies.append(accuracy_cv)\n",
    "\n",
    "# Calculate the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Fit the Super Learner on the entire training data\n",
    "super_learner.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = super_learner.predict(X_test_selected)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create and print a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean Cross-Validation Accuracy: {mean_accuracy:.2f}')\n",
    "print(f'Super Learner Test Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7326ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "\n",
    "# Selected Features\n",
    "selected_features = ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'cp_asymptomatic',\n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7',\n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping', 'ca']\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Convert 'ca' feature to float\n",
    "X_selected['ca'] = X_selected['ca'].astype(float)\n",
    "\n",
    "# Train-Test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Base Learners\n",
    "knn = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(objective=\"multi:softmax\", random_state=42)\n",
    "nn = MLPClassifier(max_iter=1000, random_state=0)\n",
    "\n",
    "# Hyperparameter Grids\n",
    "knn_param_grid = {'n_neighbors': [3, 5, 7]}\n",
    "rfc_param_grid = {'n_estimators': [100, 200, 300]}\n",
    "xgb_param_grid = {'n_estimators': [100, 200, 300]}\n",
    "nn_param_grid = {'hidden_layer_sizes': [(50, 50), (100, 100), (50, 50, 50)]}\n",
    "\n",
    "# Grid Search\n",
    "knn_grid = GridSearchCV(knn, knn_param_grid, cv=5, scoring='accuracy')\n",
    "rfc_grid = GridSearchCV(rfc, rfc_param_grid, cv=5, scoring='accuracy')\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='accuracy')\n",
    "nn_grid = GridSearchCV(nn, nn_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit base learners with hyperparameter tuning\n",
    "knn_grid.fit(X_train, y_train)\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "nn_grid.fit(X_train, y_train)\n",
    "\n",
    "# Access best hyperparameters\n",
    "best_knn_params = knn_grid.best_params_\n",
    "best_rfc_params = rfc_grid.best_params_\n",
    "best_xgb_params = xgb_grid.best_params_\n",
    "best_nn_params = nn_grid.best_params_\n",
    "\n",
    "print(knn_grid.best_params_)\n",
    "print(rfc_grid.best_params_)\n",
    "print(xgb_grid.best_params_)\n",
    "print(nn_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49180ba6",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c0764",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Selected Features\n",
    "selected_features = ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'cp_asymptomatic',\n",
    "                     'cp_atypical_angina', 'cp_non_anginal_pain', 'cp_typical_angina', 'thal_3', 'thal_7',\n",
    "                     'restecg_left_ventricular_hypertrophy', 'restecg_normal', 'slope_flat', 'slope_upsloping', 'ca']\n",
    "\n",
    "# Select the desired features from the DataFrame\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Convert 'ca' feature to float\n",
    "X_selected['ca'] = X_selected['ca'].astype(float)\n",
    "\n",
    "# Train-Test dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# Perform data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_selected = scaler.fit_transform(X_train)\n",
    "X_test_selected = scaler.transform(X_test)\n",
    "\n",
    "# Define base learners\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(learning_rate=0.1, objective=\"multi:softmax\", n_estimators=100, random_state=42)\n",
    "svm = SVC(kernel='linear', C=1, probability=True)\n",
    "nn = MLPClassifier(hidden_layer_sizes=(128, 128, 128), max_iter=1000, random_state=42)\n",
    "\n",
    "\n",
    "# Create a SuperLearner class\n",
    "class SuperLearner(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_learners):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.base_learners:\n",
    "            model.fit(X, y)\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.base_learners])\n",
    "        self.meta_learner.fit(predictions, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.base_learners])\n",
    "        return self.meta_learner.predict(predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.base_learners])\n",
    "        return self.meta_learner.predict_proba(predictions)\n",
    "\n",
    "# Initialize the Super Learner with the base learners\n",
    "super_learner = SuperLearner(base_learners=[knn, rfc, xgb, svm, nn])\n",
    "\n",
    "# Implement k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_selected):\n",
    "    X_train_cv, X_test_cv = X_train_selected[train_index], X_train_selected[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fit the Super Learner on the training data\n",
    "    super_learner.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_cv = super_learner.predict(X_test_cv)\n",
    "\n",
    "    # Calculate and store the accuracy score\n",
    "    accuracy_cv = accuracy_score(y_test_cv, y_pred_cv)\n",
    "    accuracies.append(accuracy_cv)\n",
    "\n",
    "# Calculate the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Fit the Super Learner on the entire training data\n",
    "super_learner.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = super_learner.predict(X_test_selected)\n",
    "\n",
    "# Calculate and print the accuracy score on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create and print a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "#print(f'Mean Cross-Validation Accuracy: {mean_accuracy:.2f}')\n",
    "print(f'Super Learner Test Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Convert y_test and y_pred to one-hot encoded format\n",
    "num_classes = len(np.unique(y))\n",
    "y_bin_test = label_binarize(y_test, classes=list(range(num_classes)))\n",
    "y_bin_pred = super_learner.predict_proba(X_test_selected)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin_test[:, i], y_bin_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin_test.ravel(), y_bin_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= num_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=f'macro-average ROC curve (AUC = {roc_auc[\"macro\"]:.2f})',\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green', 'red']\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Multi-Class Classification')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction with the super learner\n",
    "\n",
    "# Fit the Super Learner on the entire training data\n",
    "super_learner.fit(X_train, y_train)\n",
    "\n",
    "# Make 20 predictions on the test data\n",
    "num_predictions = 20\n",
    "predictions = np.zeros((num_predictions, len(X_test)))\n",
    "for i in range(num_predictions):\n",
    "    y_pred = super_learner.predict(X_test)\n",
    "    predictions[i] = y_pred\n",
    "\n",
    "# Create subplots for each prediction iteration\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(num_predictions):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    plt.plot(y_test, 'b.', label='Real Values')\n",
    "    plt.plot(predictions[i], 'g.', alpha=0.5, label=f'Prediction {i + 1}')\n",
    "    plt.title(f'Prediction {i + 1}')\n",
    "    plt.xlabel('Data Point')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "\n",
    "# Adjust subplot layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction with the super learner\n",
    "num_predictions = 4\n",
    "predictions = np.zeros((num_predictions, len(X_test)))\n",
    "for i in range(num_predictions):\n",
    "    y_pred = super_learner.predict(X_test)\n",
    "    predictions[i] = y_pred\n",
    "\n",
    "# Create a DataFrame to compare the first 20 predictions with actual values\n",
    "comparison_df = pd.DataFrame({'Actual': y_test[:20]})\n",
    "for i in range(num_predictions):\n",
    "    comparison_df[f'Prediction {i + 1}'] = predictions[i][:20]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43011ec0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "num_predictions = 20\n",
    "y_pred = super_learner.predict(X_test)\n",
    "\n",
    "# Plot the first prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test[:20], label='Actual', marker='o')\n",
    "plt.plot(y_pred[:20], label='Prediction 1', linestyle='--', marker='x')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Comparison of First Prediction with Actual')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb71730",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = super_learner.predict(X_test)\n",
    "actual = []\n",
    "predcition = []\n",
    "\n",
    "for i,j in zip(y_test,y_pred):\n",
    "  actual.append(i)\n",
    "  predcition.append(j)\n",
    "\n",
    "dic = {'Actual':actual,\n",
    "       'Prediction':predcition\n",
    "       }\n",
    "\n",
    "result  = pd.DataFrame(dic)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    " \n",
    "fig = go.Figure()\n",
    " \n",
    " \n",
    "fig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_test,\n",
    "                    mode='markers+lines',\n",
    "                    name='Test'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_pred,\n",
    "                    mode='markers',\n",
    "                    name='Pred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Super Learner instance with the base learners\n",
    "super_learner = SuperLearner(base_learners=[knn, rfc, xgb, svm, nn])\n",
    "\n",
    "# plot_learning_curve function\n",
    "plot_learning_curve(super_learner, \"Learning Curve\", X_train, y_train, cv=5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d8c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
